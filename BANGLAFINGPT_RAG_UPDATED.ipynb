{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9402127,"sourceType":"datasetVersion","datasetId":5707596},{"sourceId":9809531,"sourceType":"datasetVersion","datasetId":6013244},{"sourceId":9880838,"sourceType":"datasetVersion","datasetId":6066816}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip cache purge\n!pip install accelerate transformers tokenizers\n!pip install einops\n!pip install xformers\n!pip install langchain\n!pip install faiss-gpu\n!pip install sentence_transformers\n!pip install cudf\n!pip install dask-cudf\n!pip install langchain_community\n!pip install --upgrade langchain\n!pip install -U bitsandbytes","metadata":{"_uuid":"68c73827-4d41-46d7-b971-0723a0a15dba","_cell_guid":"7a72dafe-bb70-44a0-af91-5b0d1dfc227e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:36:52.855076Z","iopub.execute_input":"2024-11-13T10:36:52.855873Z","iopub.status.idle":"2024-11-13T10:40:22.236533Z","shell.execute_reply.started":"2024-11-13T10:36:52.855802Z","shell.execute_reply":"2024-11-13T10:40:22.235450Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip uninstall -y torch\n\n","metadata":{"_uuid":"dd1eb750-50dc-4c87-8629-6d4eef3c12e3","_cell_guid":"b18f7a7d-9d1e-4e20-8768-e46414a50974","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:40:22.238609Z","iopub.execute_input":"2024-11-13T10:40:22.238952Z","iopub.status.idle":"2024-11-13T10:40:25.456967Z","shell.execute_reply.started":"2024-11-13T10:40:22.238916Z","shell.execute_reply":"2024-11-13T10:40:25.455862Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.5.1\nUninstalling torch-2.5.1:\n  Successfully uninstalled torch-2.5.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"execution":{"iopub.status.busy":"2024-11-13T10:40:25.458585Z","iopub.execute_input":"2024-11-13T10:40:25.459034Z","iopub.status.idle":"2024-11-13T10:40:25.463932Z","shell.execute_reply.started":"2024-11-13T10:40:25.458987Z","shell.execute_reply":"2024-11-13T10:40:25.462994Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#!pip uninstall torch torchvision torchaudio -y\n# !pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:40:25.466432Z","iopub.execute_input":"2024-11-13T10:40:25.466755Z","iopub.status.idle":"2024-11-13T10:40:25.501031Z","shell.execute_reply.started":"2024-11-13T10:40:25.466723Z","shell.execute_reply":"2024-11-13T10:40:25.500045Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"%%capture\n%pip uninstall torchvision -y\n%pip install --pre torchvision --index-url https://download.pytorch.org/whl/nightly/cu121\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:40:25.502282Z","iopub.execute_input":"2024-11-13T10:40:25.502573Z","iopub.status.idle":"2024-11-13T10:42:29.086001Z","shell.execute_reply.started":"2024-11-13T10:40:25.502541Z","shell.execute_reply":"2024-11-13T10:42:29.084691Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"%%capture\n%pip cache purge\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:42:29.087359Z","iopub.execute_input":"2024-11-13T10:42:29.087687Z","iopub.status.idle":"2024-11-13T10:42:30.879023Z","shell.execute_reply.started":"2024-11-13T10:42:29.087652Z","shell.execute_reply":"2024-11-13T10:42:30.877949Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torchvision\nimport transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:42:30.880684Z","iopub.execute_input":"2024-11-13T10:42:30.881454Z","iopub.status.idle":"2024-11-13T10:42:35.102763Z","shell.execute_reply.started":"2024-11-13T10:42:30.881404Z","shell.execute_reply":"2024-11-13T10:42:35.101768Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"read_token='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'\nwrite_token=\"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\"","metadata":{"_uuid":"a0e480f0-45ab-4ed7-86c2-d373b3f9ad94","_cell_guid":"cb9b85e4-acf7-44b1-8a1d-9ab17d57c644","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:42:35.104279Z","iopub.execute_input":"2024-11-13T10:42:35.105181Z","iopub.status.idle":"2024-11-13T10:42:35.109478Z","shell.execute_reply.started":"2024-11-13T10:42:35.105131Z","shell.execute_reply":"2024-11-13T10:42:35.108353Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n\n# hf_token = user_secrets.get_secret(\"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\")\nlogin(token = read_token)","metadata":{"_uuid":"caf9ed7f-9ce7-49e0-b60f-06174c2d5267","_cell_guid":"73a3b6bb-7d94-4747-b99f-4a805c8e18dd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:42:35.111057Z","iopub.execute_input":"2024-11-13T10:42:35.111690Z","iopub.status.idle":"2024-11-13T10:42:35.260502Z","shell.execute_reply.started":"2024-11-13T10:42:35.111642Z","shell.execute_reply":"2024-11-13T10:42:35.259584Z"},"trusted":true},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\ntorch.cuda.empty_cache()","metadata":{"_uuid":"c4a956b5-589c-4d43-9106-636df02da68a","_cell_guid":"dbc602e4-b1e7-44d9-9fd9-11a02a382ddf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:42:35.264927Z","iopub.execute_input":"2024-11-13T10:42:35.265238Z","iopub.status.idle":"2024-11-13T10:42:35.382404Z","shell.execute_reply.started":"2024-11-13T10:42:35.265206Z","shell.execute_reply":"2024-11-13T10:42:35.381495Z"},"trusted":true},"outputs":[{"name":"stdout","text":"True\nTesla T4\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# import torch\n# import torchvision\n\n\n# print(torch._version_)\n# print(torchvision._version_)","metadata":{"_uuid":"3223c344-5ec7-43c9-a450-35d1eb634f0f","_cell_guid":"f14ad4c9-9e33-419f-9f80-b352755a99fa","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:42:35.383696Z","iopub.execute_input":"2024-11-13T10:42:35.384109Z","iopub.status.idle":"2024-11-13T10:42:35.388311Z","shell.execute_reply.started":"2024-11-13T10:42:35.384065Z","shell.execute_reply":"2024-11-13T10:42:35.387337Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from torch import cuda, bfloat16\nimport transformers\n\n# model_id = 'meta-llama/Llama-2-7b-chat-hf'\n# model_id = 'Undi95/Meta-Llama-3-8B-hf'\n# model_id = 'meta-llama/Meta-Llama-3-8B'\nmodel_id = 'Zafor158/BanglaLLama-3.2-3b-banglafingpt_finetuned-instruct-v0.0.1'\n# model_id = 'describeai/gemini'\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the bitsandbytes library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\n\n# begin initializing HF items, you need an access token\nhf_auth = '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'\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n    use_auth_token=hf_auth\n)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n#     device_map='auto',\n    device_map={'': 0},\n    use_auth_token=hf_auth\n)\n\n# enable evaluation mode to allow model inference\nmodel.eval()\n\nprint(f\"Model loaded on {device}\")","metadata":{"_uuid":"82ad034e-002c-4d79-b331-7ab6457cd249","_cell_guid":"e0f46f6d-9af9-4772-8b58-242e0b003232","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:46:40.605531Z","iopub.execute_input":"2024-11-13T10:46:40.606490Z","iopub.status.idle":"2024-11-13T10:47:36.657882Z","shell.execute_reply.started":"2024-11-13T10:46:40.606446Z","shell.execute_reply":"2024-11-13T10:47:36.656950Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b1714a4f8c349b48ed4fc10cebf046e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:182: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9532eeb53ed4b7a9420ecd9d4b14f02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85f6f9d02a8b44b08cfe54c08c8e180a"}},"metadata":{}},{"name":"stdout","text":"Model loaded on cuda:0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_auth)\n\n# Define an input prompt\n# input_text = \"ট্যাক্স কি?\"\n\n# Tokenize the input text\n# inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T10:47:36.659351Z","iopub.execute_input":"2024-11-13T10:47:36.659804Z","iopub.status.idle":"2024-11-13T10:47:38.902581Z","shell.execute_reply.started":"2024-11-13T10:47:36.659769Z","shell.execute_reply":"2024-11-13T10:47:38.901786Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20da6adfccde4cda9c0f4d736cc13a63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d1aa7f3f1f0476ea2c2af31f13c02f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/419 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ac25f5ee26482e926d039e8a4b8ee7"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# output = model.generate(\n#     inputs[\"input_ids\"],\n#     max_length=500,               # Limit response length to avoid repetition\n#     num_beams=7,                 # Higher beam width for quality\n#     temperature=0.5,             # Lower temperature for more stable output\n#     top_p=0.9,                   # Use nucleus sampling\n#     top_k=40,                    # Limit token selection to top-k choices\n#     no_repeat_ngram_size=3,      # Prevent repeating any 3-gram sequence\n#     repetition_penalty=1.5       # Higher repetition penalty\n# )\n\n# generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n# print(f\"Model Response: {generated_text}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-13T10:47:38.903826Z","iopub.execute_input":"2024-11-13T10:47:38.904149Z","iopub.status.idle":"2024-11-13T10:47:38.908542Z","shell.execute_reply.started":"2024-11-13T10:47:38.904116Z","shell.execute_reply":"2024-11-13T10:47:38.907526Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig","metadata":{"_uuid":"6531a493-fb98-4bdf-bd6d-d1daa833723a","_cell_guid":"e7a8fae2-82cd-4dfb-8841-412a773dc541","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:47:38.911041Z","iopub.execute_input":"2024-11-13T10:47:38.911907Z","iopub.status.idle":"2024-11-13T10:47:38.924366Z","shell.execute_reply.started":"2024-11-13T10:47:38.911871Z","shell.execute_reply":"2024-11-13T10:47:38.923494Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# tokenizer = LlamaTokenizer.from_pretrained(model_id)\n# # model = LlamaForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")","metadata":{"_uuid":"ef4a9586-6d55-437a-af09-f9b068e78615","_cell_guid":"6b233a34-0a50-4ead-b9d3-504af75e5880","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:47:38.925424Z","iopub.execute_input":"2024-11-13T10:47:38.925777Z","iopub.status.idle":"2024-11-13T10:47:38.931770Z","shell.execute_reply.started":"2024-11-13T10:47:38.925727Z","shell.execute_reply":"2024-11-13T10:47:38.930846Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":" #tokenizer = transformers.AutoTokenizer.from_pretrained(\n  #   model_id,\n   #  use_auth_token=read_token )","metadata":{"_uuid":"9b5d307d-cafb-4584-86f6-48c7299ad227","_cell_guid":"ece25b89-97df-45c0-9221-a8c1eea771d1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:47:38.932976Z","iopub.execute_input":"2024-11-13T10:47:38.933283Z","iopub.status.idle":"2024-11-13T10:47:38.941407Z","shell.execute_reply.started":"2024-11-13T10:47:38.933241Z","shell.execute_reply":"2024-11-13T10:47:38.940543Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"stop_list = ['\\nHuman:', '\\n```\\n']\n\nstop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\nstop_token_ids","metadata":{"_uuid":"03bf6f7a-8895-4c0e-9fcb-1e7585904aa4","_cell_guid":"1ab2b541-3cbf-4666-bae5-2ff340620966","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:47:38.942544Z","iopub.execute_input":"2024-11-13T10:47:38.943142Z","iopub.status.idle":"2024-11-13T10:47:38.956233Z","shell.execute_reply.started":"2024-11-13T10:47:38.943099Z","shell.execute_reply":"2024-11-13T10:47:38.955389Z"},"trusted":true},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[[128000, 198, 35075, 25], [128000, 198, 14196, 4077]]"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import torch\n\nstop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\nstop_token_ids","metadata":{"_uuid":"93f68709-74cf-4a04-9d94-eccef05222eb","_cell_guid":"6012e712-a953-44e4-8b8d-491d4c66749b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:47:38.957513Z","iopub.execute_input":"2024-11-13T10:47:38.957953Z","iopub.status.idle":"2024-11-13T10:47:38.966468Z","shell.execute_reply.started":"2024-11-13T10:47:38.957920Z","shell.execute_reply":"2024-11-13T10:47:38.965578Z"},"trusted":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[tensor([128000,    198,  35075,     25], device='cuda:0'),\n tensor([128000,    198,  14196,   4077], device='cuda:0')]"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from transformers import StoppingCriteria, StoppingCriteriaList\n\n# define custom stopping criteria object\nclass StopOnTokens(StoppingCriteria):\n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        for stop_ids in stop_token_ids:\n            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n                return True\n        return False\n\nstopping_criteria = StoppingCriteriaList([StopOnTokens()])","metadata":{"_uuid":"b2fa7228-ebbf-44d0-a1e6-345edfe07ee1","_cell_guid":"e276cd0b-8170-49fb-ad2c-58102ee5781c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:47:38.967782Z","iopub.execute_input":"2024-11-13T10:47:38.968168Z","iopub.status.idle":"2024-11-13T10:47:38.976572Z","shell.execute_reply.started":"2024-11-13T10:47:38.968124Z","shell.execute_reply":"2024-11-13T10:47:38.975628Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"generate_text = transformers.pipeline(\n    model=model, \n    tokenizer=tokenizer,\n    return_full_text=True,  # langchain expects the full text\n    task='text-generation',\n    # we pass model parameters here too\n    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n    max_new_tokens=512,  # max number of tokens to generate in the output\n    repetition_penalty=1.1  # without this output begins repeating\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:47:38.980157Z","iopub.execute_input":"2024-11-13T10:47:38.980594Z","iopub.status.idle":"2024-11-13T10:47:52.700675Z","shell.execute_reply.started":"2024-11-13T10:47:38.980557Z","shell.execute_reply":"2024-11-13T10:47:52.699673Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from langchain_community.llms import HuggingFacePipeline\n\nllm = HuggingFacePipeline(pipeline=generate_text)\n\n# checking again that everything is working fine\n# llm(prompt=\"Explain me the difference between Data Lakehouse and Data Warehouse.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T10:47:52.701903Z","iopub.execute_input":"2024-11-13T10:47:52.702657Z","iopub.status.idle":"2024-11-13T10:47:54.030854Z","shell.execute_reply.started":"2024-11-13T10:47:52.702592Z","shell.execute_reply":"2024-11-13T10:47:54.029672Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_145/3680895551.py:3: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=generate_text)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\nfrom langchain.schema import Document\n\n# Read the Excel file\nfile_path = \"/kaggle/input/bangla-fin-gpt-data-updated/bangla-fin-qa-data.xlsx\"\ndf = pd.read_excel(file_path)\n\n# Convert the DataFrame into a list of Documents with proper formatting\ndocuments = []\n\nfor index, row in df.iterrows():\n    # Extracting specific columns: Question, Answer, Context\n    question = str(row['Question']) if 'Question' in row else ''\n    answer = str(row['Answer']) if 'Answer' in row else ''\n    context = str(row['Context']) if 'Context' in row else ''\n\n    # Formatting the document content\n    # content = f\"Question: {question}\\nAnswer: {answer}\\nContext: {context}\"\n    content = f\"{context}\\n{question}\\n{answer}\\n\"\n\n    \n    # Create a Document object with formatted content\n    document = Document(page_content=content)\n    documents.append(document)\n\nprint(f\"Loaded {len(documents)} formatted documents from the Excel file.\")","metadata":{"_uuid":"ea6f7fd3-bf2b-4b2f-885a-a3f9937a08ea","_cell_guid":"44b529db-0d2a-4e9f-aa20-2635a3c6080d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:47:54.032358Z","iopub.execute_input":"2024-11-13T10:47:54.033124Z","iopub.status.idle":"2024-11-13T10:47:58.396779Z","shell.execute_reply.started":"2024-11-13T10:47:54.033073Z","shell.execute_reply":"2024-11-13T10:47:58.395750Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loaded 10274 formatted documents from the Excel file.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"documents[0]","metadata":{"_uuid":"c69662e3-ac53-4294-97db-b34633ab141f","_cell_guid":"aef5f420-26a3-49f9-9c16-5a350c043ee6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:47:58.397868Z","iopub.execute_input":"2024-11-13T10:47:58.398408Z","iopub.status.idle":"2024-11-13T10:47:58.405085Z","shell.execute_reply.started":"2024-11-13T10:47:58.398374Z","shell.execute_reply":"2024-11-13T10:47:58.404176Z"},"trusted":true},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Document(metadata={}, page_content='১। সংক্ষিপ্ত শিরোনাম ও প্রবর্তন। (১) এই আইন কাস্টমস আইন, ২০২৩ নামে অভিহিত হইবে। (২) সরকার, সরকারি গেজেটে প্রজ্ঞাপন দ্বারা, যেই তারিখ নির্ধারণ করিবে, সেই তারিখে এই আইন কার্যকর হইবে। ২। সংজ্ঞা। বিষয় বা প্রসঙ্গের পরিপন্থি কোনো কিছু না থাকিলে, এই আইনে- (১) “আপিল ট্রাইব্যুনাল” অর্থ ধারা ২২৫ এর অধীন গঠিত কাস্টমস, এক্সাইজ এবং মূল্য সংযোজন কর আপিল ট্রাইব্যুনাল; (২) আানদানি অর্থ বিদেশ হইতে কোনো পণ্য বাংলাদেশে আনয়ন করা; (৩) \"আমদানিকারক\" অর্থ এইরূপ কোনো ব্যক্তি যিনি স্বয়ং বা যাহার পক্ষে কোনো পণ্য আমদানি করা হয়, এবং উক্ত পণ্য আমদানির সময় হইতে কাস্টমস আনুষ্ঠানিকতা সমাপ্তির পূর্ব পর্যন্ত উহার মালিক বা দখলের অধিকার বা স্বার্থ রহিয়াছে এইরূপ প্রাপক (consignee) বা ব্যক্তিও ইহার অন্তর্ভুক্ত হইবে; (৪) \"আমদানিকৃত অর্থ বিদেশ হইতে বাংলাদেশে আনীত হইয়াছে বা প্রবেশ করানো হইয়াছে এইরূপ কোনো পণ্য, এবং বাংলাদেশে আনীত বা আগত কোনো পরিত্যক্ত (derelict) পণ্য, জাহাজ হইতে নিক্ষিপ্ত (jetsam) পণ্য, ডুবন্ত জাহাজের ভাসমান (flotsam) পণ্য বা উহার ধ্বংসাবশেষও (wreck) ইহার অন্তর্ভুক্ত হইবে; (৫) \"আমদানি শুল্ক ও কর অর্থ পণ্য আমদানির সহিত সম্পর্কিত বা আমদানিকৃত পণ্যের উপর ধারা ১৮, ১৯, ২০ ও ২৩ এর অধীন আরোপণীয়, ক্ষেত্রমত, কাস্টমস শুল্ক ও অন্য কোনো শুল্ক, কর বা চার্জ এবং রেগুলেটরি, কাউন্টারভেইলিং, অ্যান্টি- ডাম্পিং ও সেইফগার্ড শুল্ক; তবে ধারা ২৪ এর অধীন প্রদত্ত কোনো সেবার জন্য ফি বা কোনো সরকারি কর্তৃপক্ষের পক্ষে বোর্ড কর্তৃক সংগৃহীত কোনো চার্জ ইহার অন্তর্ভুক্ত হইবে না; (৬) উপকূলীয় পন্ড অর্থ কাস্টমস শুল্ক পরিশোধিত হয় নাই এইরূপ আমদানিকৃত পণ্য ব্যতীত বাংলাদেশের এক বন্দর হইতে অন্য বন্দরে নৌযানযোগে পরিবহণকৃত পণ্য; (৭) \"উপযুক্ত কর্তৃপক্ষ” অর্থ পণ্য আমদানি বা রপ্তানি সম্পর্কিত কার্যাবলি সম্পাদনের জন্য আইন দ্বারা ক্ষমতাপ্রাপ্ত কোনো সরকারি সংস্থা; (৮) \"এজেন্ট\" অর্থ শিপিং এজেন্ট, ক্লিয়ারিং এন্ড ফরোয়ার্ডিং এজেন্ট, কার্গো এজেন্ট এবং ফ্রেইট ফরোয়ার্ডিং এজেন্টসহ ধারা ২৪৩ এর অধীন লাইসেন্সপ্রাপ্ত কোনো ব্যক্তি অথবা ধারা ২৪৪ এর অধীন কার্যাবলি পরিচালনা করিবার জন্য অনুমতিপ্রাপ্ত কোনো ব্যক্তিও ইহার অন্তর্ভুক্ত হইবে; (৯) \"ওয়্যারহাউস\" অর্থ ধারা ১১ এর অধীন ঘোষিত বা ধারা ১২ এর অধীন লাইসেন্সপ্রাপ্ত কোনো স্থান; (১০) \"ওয়্যারহাউসিং স্টেশন\" অর্থ ধারা ১০ এর অধীন ওয়্যারহাউসিং স্টেশন হিসাবে ঘোষিত কোনো স্থান; (১১) \"কার্গো ঘোষণা\" অর্থ ধারা ৪৮ বা, ক্ষেত্রমত, ধারা ৫৫ এর অধীন প্রদত্ত কোনো কার্গো ঘোষণা; (১২) \"কাস্টমস অভ্যন্তরীণ কন্টেইনার ডিপো\" অর্থ ধারা ৮ এর অধীন কাস্টমস অভ্যন্তরীণ কন্টেইনার ডিপো হিসাবে ঘোষিত কোনো এলঢঢাকা; (১৩) \"কাস্টমস অভ্যন্তরীণ নৌ-কন্টেইনার টার্মিনাল\" অর্থ ধারা ৮ এর অধীন কাস্টমস অভ্যন্তরীণ নৌ-কন্টেইনার টার্মিনাল হিসাবে ঘোষিত কোনো এলঢঢাকা; (১৪) \"ফান্টজন এলঢঢাকা অর্থ ধারা ৯ এর অধীন নির্ধারিত কাস্টমস স্টেশনের সীমা, এবং আমদানিকৃত বা রপ্তানির জন্য কোনো পণ্য কাস্টমস কর্তৃপক্ষ কর্তৃক ছাড় প্রদানের পূর্বে যে এলঢঢাকায় সাধারণত রক্ষিত থাকে সেই এলঢঢাকাও ইহার অন্তর্ভুক্ত হইবে; (১৫) \"কাস্টমস কম্পিউটার সিস্টেন\" অর্থ বোর্ড কর্তৃক প্রতিষ্ঠিত বা নিয়োজিত কোনো কাস্টমস কম্পিউটারাইজড প্রক্রিয়াকরণ পদ্ধতি, যাহা এতদ্\\u200cসংশ্লিষ্ট অন্যান্য পদ্ধতির সহিত উপযুক্ত বা প্রয়োজনীয়ভাবে আন্তঃসংযোগকৃত; (১৬) \"কাস্টমস কর্মকর্তা\" অর্থ ধারা ৪ এর অধীন নিয়োগপ্রাপ্ত কোনো কাস্টমস কর্মকর্তা; (১৭) \"কান্টনস বিষানবন্দর অর্থ ধারা ৮ এর অধীন কাস্টমস বিমানবন্দর হিসাবে ঘোষিত কোনো বিমানবন্দর; (১৮) \"কাস্টমস নিয়ন্ত্রণ অর্থ বাংলাদেশ ও অন্যান্য দেশ বা অঞ্চলের মধ্যে পণ্য আমদানি, রপ্তানি, ট্রানজিট, স্থানান্তর ও মজুদ এবং আমদানিকৃত পণ্যের অবস্থান ও স্থানান্তর সম্পর্কিত এই আইনের বিধানসমূহের প্রতিপালন নিশ্চিত করিবার উদ্দেশ্যে কাস্টমস কর্মকর্তা কর্তৃক গৃহীত কোনো কার্যক্রম; (১৯) \"কান্টসস পদ্ধতি\" অর্থ কাস্টমস বিষয়ক নিম্নবর্ণিত যে কোনো পদ্ধতি, যথা:- (ক) দেশীয় ভোগের জন্য ছাড় প্রদান; (খ) সাময়িক আমদানি; (গ) ইনওয়ার্ড প্রসেসিং; (ঘ) আউটওয়ার্ড প্রসেসিং; (ঙ) কাস্টমস ওয়্যারহাউসিং; (চ) ট্রানজিট; (ছ) ট্রান্সশিপমেন্ট; (জ) রসদ ও ভান্ডার সামগ্রী; (ঝ) রপ্তানি; বা (ঞ) বোর্ড কর্তৃক, সরকারি গেজেটে প্রজ্ঞাপন দ্বারা, নির্ধারিত অন্য কোনো পদ্ধতি; (২০) \"কাস্টমস বম্বর\" অর্থ ধারা ৮ এর অধীন কাস্টমস বন্দর হিসাবে ঘোষিত কোনো এলঢঢাকা; (২১) কাটনল দুল্ট (assessable value)\" অর্থ ধারা ২৭ অনুযায়ী নিরূপিত মূল্য, যাহা কোনো পণ্যের উপর কাস্টমস শুল্ক আরোপণের ভিত্তি; (২২) কান্টমস শুল্ক\\' অর্থ প্রথম তফসিলে উল্লিখিত কোনো শুল্ক, অথবা বাংলাদেশে পণ্য প্রবেশ বা প্রস্থান সংক্রান্ত প্রচলিত অন্য কোনো আইনের অধীন প্রদেয় কোনো শুল্ক; (২৩) \"কান্টনন স্টেশন অর্থ ধারা ৮ এর অধীন, সময় সময়, ঘোষিত কোনো কাস্টমস কদর, কাস্টমস বিমানবন্দর, স্থল কাস্টমস স্টেশন, কাস্টমস অভ্যন্তরীণ কন্টেইনার ডিপো, কাস্টমস অভ্যন্তরীণ নৌ-কনটেইনার টার্মিনাল, বা অনুরূপ অন্য কোনো এলঢঢাকা; (২৪) \"চোরাচালান\" অর্থ আপাতত বলবৎ কোনো আইনের অধীন আরোপিত কোনো নিষেধাজ্ঞা বা বিধি-নিষেধ লঙ্ঘন করিয়া অথবা আরোপণীয় কাস্টমস শুল্ক বা কর ফাঁকি দিয়া নিম্নবর্ণিত কোনো পণ্য বাংলাদেশের অভ্যন্তরে আনয়ন করা বা বাহিরে লইয়া যাওয়া, যথা (ক) মাদক দ্রব্য, নেশাজাতীয় ঔষধ বা সাইকোট্রপিক বস্তু; (খ) স্বর্ণবার, রৌপ্যবার, প্লাটিনাম, প্যালাডিয়াম, রেডিয়াম, মহামূল্যবান পাথর, মুদ্রা অথবা স্বর্ণ, রৌপ্য, প্লাটিনাম, প্যালাডিয়াম বা কোনো মহামূল্যবান পাথরের তৈরিবস্তু অথবা সরকার কর্তৃক, সময় সময়, সরকারি গেজেটে প্রজ্ঞাপন দ্বারা নির্ধারিত মূল্যের অধিক মূল্যমানের কোনো পণ্য; (গ) কোনো জাহাজ, নৌযান, উড়োজাহাজ বা অন্য কোনো যানবাহনের কোনো স্থানে অথবা কোনো ব্যাগেজ বা কোনো পণ্যের মধ্যে বা কোনো ব্যক্তির দেহে যে কোনো প্রকারে লুকানো (concealed) কোনো পণ্য; বা (ঘ) ধারা ৮ বা ধারা ৯ এর অধীন ঘোষিত কোনো রুট ব্যতীত অন্য কোনো রুটে (route) কাস্টমস স্টেশন ব্যতীত অন্য কোনো স্থান হইতে আনীত বা বাহিরে লওয়া কোনো পণ্য; এবং উক্ত পণ্য উল্লিখিতভাবে আনয়ন করিবার বা বাহিরে নেওয়ার জন্য কোনো প্রচেষ্টা, প্ররোচনা বা সমর্থনও উহার অন্তর্ভুক্ত হইবে এবং সকল সমজাতীয় শব্দ ও অভিব্যক্তিসমূহের ব্যাখ্যা তদনুসারে হইবে; বা (ঙ) বোর্ড কর্তৃক, সরকারি গেজেটে প্রজ্ঞাপন দ্বারা, নির্ধারিত অন্য কোনো পণ্য; (২৫) \"ছোট (wharf)\" অর্থ ধারা ৯ এর দফা (খ) এর অধীন কাস্টমস বন্দরে পণ্য বা কোনো পণ্যশ্রেণি বোঝাই ও খালাস করিবার জন্য অনুমোদিত কোনো স্থান; (২৬) দলবিধি অর্থ Penal Code, 1860 (Act No. XLV of 1860); (২৭) \"দেওয়ানি কারীনি অর্থ Code of Civil Procedure, 1908 (Act No. V of 1908); (২৮) \"নির্ধারিত\" অর্থ বিধি বা, ক্ষেত্রমত, আদেশ বা প্রজ্ঞাপন দ্বারা নির্ধারিত; (২৯) ন্যাজনির্ণয়ন (adjudication)\" অর্থ জরিমানা আরোপযোগ্য কাস্টমস অপরাধ বিষয়ে যথাযথ কর্মকর্তা কর্তৃক গৃহীত প্রশাসনিক কার্যক্রম; (৩০) অর্থ যে কোনো অস্থাবর সম্পত্তি এবং নিম্নবর্ণিত পণ্যও ইহার অন্তর্ভুক্ত হইবে, যথা:- (ক) যানবাহন। (খ) রসদ ও ভান্ডার সামগ্রী। (গ) ব্যাগেজ: (ঘ) ইলেক্ট্রনিক ডাটা; (ঙ) মুদ্রা এবং বিনিময়যোগ্য দলিলপত্র (negotiable instruments); এবং (চ) বোর্ড কর্তৃক, সরকারি গেজেটে প্রজ্ঞাপন দ্বারা, ঘোষিত অন্য কোনো পণ্য: (৩১) পণ্য ঘোষণা অর্থ ধারা ৮১ এর বিধান অনুযায়ী প্রদত্ত কোনো পণ্যের ঘোষনা; (৩২) পণ্যের শ্রেণিশিয়ান (H.S. Classification)\" অর্থ World Customs Organization (WCO) কর্তৃক উদ্ভাবিত Harmonized Commodity Description and Coding System অনুসরণপূর্বক প্রণীত, এই আইনের প্রথম তফসীল অনুযায়ী পণ্যের শ্রেণিবিন্যাস; (৩৩) সভাপতি অর্থ আপিল ট্রাইব্যুনালের সভাপতি; (৩৪) ফৌজদারি কার্যবিধি অর্থ Code of Criminal Procedure, 1898 (Act No. V of 1898); (৩৫) বাংলাদেশ ফান্টমস অলসীমা অর্থ বাংলাদেশের যথোপযুক্ত উপকূলের তটরেখা হইতে পরিমাপকৃত ২৪ (চব্বিশ) নটিক্যাল মাইল দূরত্ব পর্যন্ত সমুদ্রের মধ্যে বিস্তৃত জলসীমা; (৩৬) বাংলাদেশে প্রভিতিত প্রতিষ্ঠান\" অর্থ বাংলাদেশের কোনো স্থায়ী অধিবাসী এবং বাংলাদেশে অবস্থানের অনুমতিপ্রাপ্ত বা আইনগতভাবে ক্ষমতাপ্রাপ্ত ও নিবন্ধিত প্রতিষ্ঠান, কোম্পানি বা ব্যক্তিসংঘ; (৩৭) \"বিনি\" অর্থ এই আইনের অধীন প্রণীত কোনো বিধি; (৩৮) জেত অর্থ National Board of Revenue Order, 1972 (President\\'s Order No. 76 of 1972) এর অধীন গঠিত জাতীয় রাজস্ব বোর্ড; (৩৯) শুক্তি\" অর্থে কোনো কোম্পানি, অংশীদারিত্বমূলক কারবার, প্রতিষ্ঠান, সংস্থা, সমিতি বা ব্যক্তি সংঘও উহার অন্তর্ভুক্ত হইবে; (৪০) ভারপ্রাপ্ত কক্তি অর্থ (ক) নৌযানের ক্ষেত্রে, মাস্টার; (খ) উড়োজাহাজের ক্ষেত্রে, কমান্ডার বা পাইলট; (গ) রেলওয়ে ট্রেনের ক্ষেত্রে, কন্ডাক্টর, পরিচালক বা পরিচালক হিসাবে নিয়োজিত কোনো ব্যক্তি: বা (ঘ) অন্যান্য যানবাহনের ক্ষেত্রে, চালক বা উহার নিয়ন্ত্রণকারী; (৪১) মাল্টার অর্থ, নৌযানের ক্ষেত্রে, পাইলট বা হারবার মাস্টার ব্যতীত উক্ত নৌযানের উপর কর্তৃত্ব বা দায়িত্ব রহিয়াছে এইরূপ কোনো ব্যক্তি; (৪২) \"বর্ষাবর্ষ কর্মকর্তা (appropriate officer)\" অর্থ এই আইনের অধীন কোনো দায়িত্ব পালনের ক্ষেত্রে, এই আইনের দ্বারা বা অধীন, উক্ত দায়িত্ব পালনের জনা দায়িত্বপ্রাপ্ত কোনো কাস্টমস কর্মকর্তা। (৪৩) \"বানবাহন\" অর্থ জলে, স্থলে বা আকাশপথে পণ্য বা যাত্রী পরিবহণের জন্য ব্যবহৃত যে কোনো প্রকারের যান্ত্রিক ও অযান্ত্রিক যানবাহন; (88) \"রপ্তানি শুল্ক ও কর অর্থ পণ্য রপ্তানির সহিত সম্পর্কিত বা রপ্তানিতবা পণ্যের উপর আরোপনীয় কাস্টমস শুল্কসহ অন্য সকল প্রকারের শুল্ক, কর বা চার্জ, তবে ধারা ২৪ এর অধীন প্রদত্ত কোনো সেবার জন্য ফি বা অন্য কোনো সরকারি কর্তৃপক্ষের পক্ষে বোর্ড কর্তৃক সংগৃহীত চার্জ উহার অন্তর্ভুক্ত হইবে না। (৪৫) রপ্তানিকারক অর্থ এইরূপ কোনো ব্যক্তি যিনি স্বয়ং বা যাহার পক্ষে কোনো পণ্য রপ্তানি করা হয়, এবং উক্ত পণ্য রপ্তানির ঘোষণা প্রদানের পর এবং রপ্তানির পূর্ব পর্যন্ত উহার মালিক বা দখলের অধিকার বা স্বার্থ রহিয়াছে বা থাকিবে, এইরূপ প্রেরক (consignor) বা ব্যক্তিও উহার অন্তর্ভুক্ত হইবে; (৪৬) রপ্তানি কার্গো ঘোষণা অর্থ ধারা ৫৫ এর অধীন প্রদত্ত কোনো রপ্তানি কার্গো ঘোষণা; (৪৭) রসদ ও ভান্ডার অর্থ কোনো যানবাহনে ব্যবহারের জন্য বা সঞ্চিত কোনো পণ্য এবং, তাৎক্ষণিকভাবে ব্যবহারের জন্য হউক বা না হউক, জ্বালানি, খুচরা যন্ত্রাংশ ও অন্যান্য উপকরণ এবং যানবাহনের যাত্রীদের নিকট খুচরা বিক্রয়ের উদ্দেশ্যে উক্ত যানবাহনে বহনকৃত অন্য কোনো পণ্যও উহার অন্তর্ভুক্ত হইবে; (৪৮) \"বুক ও কর অর্থ আমদানি শুল্ক ও কর অথবা রপ্তানি শুল্ক ও কর; (৪৯) মূল কাস্টমস স্টেশন অর্থ ধারা ৮ এর অধীন অভ্যন্তরীণ নদী কন্দরসহ স্থল কাস্টমস স্টেশন হিসাবে ঘোষিত কোনো স্থান।\\nকাস্টমস কি?\\nকাস্টমস হলো এমন একটি স্থান বা সংস্থা যেখানে বন্দরে পণ্য বা কোনো পণ্যশ্রেণি বোঝাই এবং খালাস করিবার জন্য অনুমোদিত কাস্টমস কর্মকর্তারা কাজ করে।\\n')"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\nall_splits = text_splitter.split_documents(documents)","metadata":{"_uuid":"beb84985-dc47-4718-a502-4fa508a9c600","_cell_guid":"2016a505-3abb-4152-babc-5e4601da5b98","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:47:58.406251Z","iopub.execute_input":"2024-11-13T10:47:58.406544Z","iopub.status.idle":"2024-11-13T10:48:17.136310Z","shell.execute_reply.started":"2024-11-13T10:47:58.406512Z","shell.execute_reply":"2024-11-13T10:48:17.135463Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\n\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\"\nmodel_kwargs = {\"device\": \"cuda\"}\n\nembeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n# storing embeddings in the vector store\nvectorstore = FAISS.from_documents(all_splits, embeddings)","metadata":{"_uuid":"f2c43d39-29c3-4556-9e4c-86ab43367def","_cell_guid":"f406cbed-4619-46cc-8981-9b429e7c1ea2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-13T10:48:17.137535Z","iopub.execute_input":"2024-11-13T10:48:17.137947Z","iopub.status.idle":"2024-11-13T11:23:47.577575Z","shell.execute_reply.started":"2024-11-13T10:48:17.137901Z","shell.execute_reply":"2024-11-13T11:23:47.576474Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_145/1881137274.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7ddd5e3dada40e58b972a06b31c1085"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df136d18a93410e85a23a361f5ce341"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b9304258580470f9b2d59bb58a83ead"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"711503c9f0c24053b4002bd99cd2c82e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e8b59c5e7304e25bc42601f985ed64c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44fe17569be3472b9779eddd5939e33c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9290fc9962af43d0b93be14b51b51230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f4bdb879474fa1965ef344f952b57c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df1b5dd8481480aa49ccf82a5aa66b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72d03335df8744a1b3f3fb04c78f4295"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f926c203c1d544348f061b072b0e38cf"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"def retrieve_documents(query, top_k=5):\n    query_embedding = embeddings.embed_query(query)  # Get embedding for the query\n    results = vectorstore.similarity_search_by_vector(query_embedding, top_k)  # Retrieve top-k documents\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.582508Z","iopub.execute_input":"2024-11-13T11:23:47.583185Z","iopub.status.idle":"2024-11-13T11:23:47.588400Z","shell.execute_reply.started":"2024-11-13T11:23:47.583148Z","shell.execute_reply":"2024-11-13T11:23:47.587324Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def generate_answer(query, top_k=5, max_length=500):\n    # Retrieve relevant documents from FAISS\n    retrieved_docs = retrieve_documents(query, top_k)\n\n    # Combine the retrieved documents into a context string for the model\n    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n\n    # Tokenize the query and context to feed into the model\n    input_text = f\"{context}\\nQuery: {query}\\nAnswer:\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.589679Z","iopub.execute_input":"2024-11-13T11:23:47.590081Z","iopub.status.idle":"2024-11-13T11:23:47.599562Z","shell.execute_reply.started":"2024-11-13T11:23:47.590009Z","shell.execute_reply":"2024-11-13T11:23:47.598678Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# # Generate the answer using the fine-tuned model\n# output = model.generate(\n#     inputs[\"input_ids\"],\n#     max_length=2000,\n#     num_beams=7,\n#     temperature=0.5,\n#     top_p=0.9,\n#     top_k=40,\n#     no_repeat_ngram_size=3,\n#     repetition_penalty=1.5,\n#     stopping_criteria=stopping_criteria  # Custom stopping criteria defined earlier\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.600727Z","iopub.execute_input":"2024-11-13T11:23:47.601134Z","iopub.status.idle":"2024-11-13T11:23:47.610191Z","shell.execute_reply.started":"2024-11-13T11:23:47.601102Z","shell.execute_reply":"2024-11-13T11:23:47.609291Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ndef generate_response(input_text):\n    # Tokenize the input text\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n\n    # Generate the answer using the fine-tuned model\n    output = model.generate(\n        inputs[\"input_ids\"],\n        max_length=2000,\n        num_beams=7,\n        temperature=0.5,\n        top_p=0.9,\n        top_k=40,\n        no_repeat_ngram_size=3,\n        repetition_penalty=1.5,\n        stopping_criteria=stopping_criteria  # Custom stopping criteria defined earlier\n    )\n\n    # Decode the output to get the generated response\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return generated_text  # Return the generated text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.611182Z","iopub.execute_input":"2024-11-13T11:23:47.611432Z","iopub.status.idle":"2024-11-13T11:23:47.620656Z","shell.execute_reply.started":"2024-11-13T11:23:47.611404Z","shell.execute_reply":"2024-11-13T11:23:47.619744Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def generate_answer(query, top_k=5, max_length=500):\n    # Retrieve relevant documents from FAISS\n    retrieved_docs = retrieve_documents(query, top_k)\n\n    # Combine the retrieved documents into a context string for the model\n    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n\n    # Tokenize the query and context to feed into the model\n    input_text = f\"{context}\\nQuery: {query}\\nAnswer:\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n\n    # Generate the answer using the fine-tuned model\n    output = model.generate(\n        inputs[\"input_ids\"],\n        max_length=1000,\n        num_beams=7,\n        temperature=0.5,\n        top_p=0.9,\n        top_k=40,\n        no_repeat_ngram_size=3,\n        repetition_penalty=1.5,\n    )\n\n    # Decode the output to get the generated response\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n    return generated_text  # Return the generated text\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.621764Z","iopub.execute_input":"2024-11-13T11:23:47.622050Z","iopub.status.idle":"2024-11-13T11:23:47.635781Z","shell.execute_reply.started":"2024-11-13T11:23:47.622020Z","shell.execute_reply":"2024-11-13T11:23:47.634850Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Example of how to call the function\ninput_text = \"কীভাবে ইলেকট্রনিক রেকর্ড সংরক্ষণ করা হয়?\"\n# response = generate_answer(input_text)\n# print(f\"Model Response: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.636991Z","iopub.execute_input":"2024-11-13T11:23:47.637257Z","iopub.status.idle":"2024-11-13T11:23:47.646212Z","shell.execute_reply.started":"2024-11-13T11:23:47.637229Z","shell.execute_reply":"2024-11-13T11:23:47.645476Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# import re\n\n# def extract_content(text, section):\n#     \"\"\"\n#     Extracts the content of the specified section ('Helpful Answer' or 'Answer') from the given text.\n    \n#     Parameters:\n#         text (str): The input text containing the sections.\n#         section (str): The section to extract ('Helpful Answer' or 'Answer').\n        \n#     Returns:\n#         str: The content of the specified section or an empty string if the section is not found.\n#     \"\"\"\n#     # Define the pattern to match the section and capture the content after it\n#     pattern = rf\"{section}:\\s*(.+)\"\n    \n#     # Use regular expression to search for the section content\n#     match = re.search(pattern, text, re.IGNORECASE)\n    \n#     # Return the content if found, otherwise return an empty string\n#     return match.group(1).strip() if match else \"\"\n\n# # Example usage","metadata":{"_uuid":"07899d6c-0d7b-429e-bb0b-7eb2e8421483","_cell_guid":"1111103c-ff23-4f43-91f5-951e03c5c087","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.647233Z","iopub.execute_input":"2024-11-13T11:23:47.647517Z","iopub.status.idle":"2024-11-13T11:23:47.657722Z","shell.execute_reply.started":"2024-11-13T11:23:47.647487Z","shell.execute_reply":"2024-11-13T11:23:47.656817Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from langchain.chains import ConversationalRetrievalChain\n\nchain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)","metadata":{"_uuid":"b3aba102-281c-4a61-9ba5-54c436014005","_cell_guid":"39846d93-12bd-43bc-8b20-5a7cca38a0c0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.658821Z","iopub.execute_input":"2024-11-13T11:23:47.659109Z","iopub.status.idle":"2024-11-13T11:23:47.921854Z","shell.execute_reply.started":"2024-11-13T11:23:47.659078Z","shell.execute_reply":"2024-11-13T11:23:47.921058Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"chat_history = []\n\nquery = \"\"\"\n# Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# Keep the response in details and precise. Don't repeat same words or lines repetadely. The response should have to be meaningful. The content under Input is the main Question. You should not repeat anythings from the prompt or instruction that I give you. Follow the instruction and input but don't repeat the words written in it. You should answer the question In Bangla. The response have to be in Bangla.\n\n# ### Input:\n# What is the process for filing income tax returns for individual taxpayers?\n\n# \"\"\"\n #result = chain({\"question\": query, \"chat_history\": chat_history})\n#result = chain({\"question\": query, \"chat_history\": chat_history})\n\n\n#print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"1665bb7e-65be-4ab8-893b-bf25e0f9aabe","_cell_guid":"f89f690b-b574-4d0b-98a2-7a1365251476","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.923188Z","iopub.execute_input":"2024-11-13T11:23:47.923514Z","iopub.status.idle":"2024-11-13T11:23:47.928412Z","shell.execute_reply.started":"2024-11-13T11:23:47.923480Z","shell.execute_reply":"2024-11-13T11:23:47.927430Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# Keep the response in details and precise. Don't repeat same words or lines repetadely. The response should have to be meaningful. The content under Input is the main Question. You should not repeat anythings from the prompt or instruction that I give you. Follow the instruction and input but don't repeat the words written in it. You should answer the question In Bangla. The response have to be in Bangla.\n\n# ### Input:\n# হাইকোর্ট বিভাগে শুনানী গ্রহণের জন্য বিচারকগণ কে কে??\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# result['answer']","metadata":{"_uuid":"1b34c2b6-00a4-48ba-b778-45b14c49cd00","_cell_guid":"79ce3627-d457-4b0d-a1c6-7b1385a5dee8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.929673Z","iopub.execute_input":"2024-11-13T11:23:47.929968Z","iopub.status.idle":"2024-11-13T11:23:47.941217Z","shell.execute_reply.started":"2024-11-13T11:23:47.929937Z","shell.execute_reply":"2024-11-13T11:23:47.940423Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# Keep the response in details and precise. Don't repeat same words or lines repetadely. The response should have to be meaningful. The content under Input is the main Question. You should not repeat anythings from the prompt or instruction that I give you. Follow the instruction and input but don't repeat the words written in it. You should answer the question In Bangla. The response have to be in Bangla. Don't use English Language for response. Use Bangla Language for response.\n\n# ### Input:\n# In what activities can customs officers be engaged?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"90e25555-f8c0-4cd7-bb79-e92ef915b36c","_cell_guid":"9041c6e1-6dfe-46f6-aa75-969fb9254207","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.942433Z","iopub.execute_input":"2024-11-13T11:23:47.943239Z","iopub.status.idle":"2024-11-13T11:23:47.953567Z","shell.execute_reply.started":"2024-11-13T11:23:47.943181Z","shell.execute_reply":"2024-11-13T11:23:47.952780Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# Keep the response in details and precise. Don't repeat same words or lines repetadely. The response should have to be meaningful. The content under Input is the main Question. You should not repeat anythings from the prompt or instruction that I give you. Follow the instruction and input but don't repeat the words written in it. You should answer the question In Bangla. The response have to be in Bangla. Don't use English Language for response. Use Bangla Language for response.\n\n# ### Input:\n# হাইকোর্ট বিভাগে শুনানী গ্রহণের জন্য বিচারকগণ কে কে?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"d781b947-0017-42b3-b64c-b12ca51fe6f2","_cell_guid":"d9c93c7a-ade8-48f0-9cf0-f7b0b1b768bd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.956419Z","iopub.execute_input":"2024-11-13T11:23:47.957101Z","iopub.status.idle":"2024-11-13T11:23:47.963180Z","shell.execute_reply.started":"2024-11-13T11:23:47.957066Z","shell.execute_reply":"2024-11-13T11:23:47.962246Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# ব্যক্তিগত করদাতাদের জন্য আয়কর রিটার্ন দাখিল করার প্রক্রিয়া কি?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"f3bdd7b3-aa60-4be8-a78b-71a5b4819c0c","_cell_guid":"b987b87e-f7e0-4ed4-972b-1af7f4833455","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.969536Z","iopub.execute_input":"2024-11-13T11:23:47.970269Z","iopub.status.idle":"2024-11-13T11:23:47.976559Z","shell.execute_reply.started":"2024-11-13T11:23:47.970227Z","shell.execute_reply":"2024-11-13T11:23:47.975668Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# কিভাবে কোনো কাস্টমস কর্মকর্তা অন্য কোনো সরকারি কর্মকর্তার উপর অর্পণ করিতে পারে?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"1b718302-120f-4534-b713-c196592e5d25","_cell_guid":"7cbd7d4f-b1ba-4066-81e0-5fa221634275","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.977851Z","iopub.execute_input":"2024-11-13T11:23:47.978504Z","iopub.status.idle":"2024-11-13T11:23:47.989703Z","shell.execute_reply.started":"2024-11-13T11:23:47.978460Z","shell.execute_reply":"2024-11-13T11:23:47.988898Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# কাস্টমস কর্মকর্তা হিসেবে কাজ করতে আগ্রহী হতে হলে কি যোগ্যতা ও অভিজ্ঞতা প্রয়োজন?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))\n# # result['answer']","metadata":{"_uuid":"e5d82a71-ebb5-417f-bc52-6f1f61e08373","_cell_guid":"69e65aaa-14bb-4450-85ce-681b66a27091","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:47.990957Z","iopub.execute_input":"2024-11-13T11:23:47.991786Z","iopub.status.idle":"2024-11-13T11:23:48.003489Z","shell.execute_reply.started":"2024-11-13T11:23:47.991737Z","shell.execute_reply":"2024-11-13T11:23:48.002667Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# ট্র্যানজ্যাকশন বেইজড অডিটের প্রথম ধাপে কী করতে হবে?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))","metadata":{"_uuid":"052dc722-54a5-4dde-af4a-4509a0d380a1","_cell_guid":"e9f755d7-9e43-413d-b124-8a94f040682c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:48.004710Z","iopub.execute_input":"2024-11-13T11:23:48.005052Z","iopub.status.idle":"2024-11-13T11:23:48.016931Z","shell.execute_reply.started":"2024-11-13T11:23:48.004995Z","shell.execute_reply":"2024-11-13T11:23:48.016094Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# query = \"\"\"\n# Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n# ### Instruction:\n# For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n# ### Input:\n# ট্র্যানজ্যাকশন বেইজড অডিটের প্রথম ধাপে কী করতে হবে?\n\n# \"\"\"\n# result = chain({\"question\": query, \"chat_history\": chat_history})\n\n# print(extract_content(result['answer'], 'Helpful Answer'))","metadata":{"_uuid":"f75f5adf-2581-420b-a2a8-0ff0d3cdefd0","_cell_guid":"f21c0e66-988c-415f-951b-5bb86eabb938","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:48.018303Z","iopub.execute_input":"2024-11-13T11:23:48.018676Z","iopub.status.idle":"2024-11-13T11:23:48.031209Z","shell.execute_reply.started":"2024-11-13T11:23:48.018634Z","shell.execute_reply":"2024-11-13T11:23:48.030363Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"!pip install -q streamlit","metadata":{"_uuid":"0cb34dc0-ba5d-41bd-9176-6698bd6dabff","_cell_guid":"93f2adaf-5685-4b48-89dc-1440c6ae58ac","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:23:48.032185Z","iopub.execute_input":"2024-11-13T11:23:48.032465Z","iopub.status.idle":"2024-11-13T11:24:01.517675Z","shell.execute_reply.started":"2024-11-13T11:23:48.032434Z","shell.execute_reply":"2024-11-13T11:24:01.516422Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"\n\n# Your chain function and extract_content should already be defined\n# For example:\n# from your_chain_module import chain, extract_content\n\ndef generate_prompt(user_question):\n    query_template = \"\"\"\n    Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n    ### Instruction:\n    For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n    ### Input:\n    {}\n    \"\"\"\n    # Replace {} with the dynamic user question\n    return query_template.format(user_question)","metadata":{"_uuid":"1c6d28b2-26e8-4894-965a-684a04194b6f","_cell_guid":"fd102196-5f4e-4177-a336-e517bca2b1f6","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:01.519324Z","iopub.execute_input":"2024-11-13T11:24:01.519684Z","iopub.status.idle":"2024-11-13T11:24:01.526129Z","shell.execute_reply.started":"2024-11-13T11:24:01.519649Z","shell.execute_reply":"2024-11-13T11:24:01.525175Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"!pip install --no-dependencies --quiet streamlit","metadata":{"_uuid":"3a712ad3-2ceb-43d9-b878-b4b0b16a3b59","_cell_guid":"b4a414a5-c2bc-4875-9454-14667cbd4044","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:01.527663Z","iopub.execute_input":"2024-11-13T11:24:01.528090Z","iopub.status.idle":"2024-11-13T11:24:03.601952Z","shell.execute_reply.started":"2024-11-13T11:24:01.528047Z","shell.execute_reply":"2024-11-13T11:24:03.600691Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# %%writefile app.py\n# import streamlit as st\n# from huggingface_hub import login\n# from kaggle_secrets import UserSecretsClient\n# import re\n# import torch\n# from torch import cuda, bfloat16\n# import transformers\n# from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig\n# from langchain_community.llms import HuggingFacePipeline\n# import pandas as pd\n# from langchain.schema import Document\n\n# def extract_content(text, section):\n#     \"\"\"\n#     Extracts the content of the specified section ('Helpful Answer' or 'Answer') from the given text.\n    \n#     Parameters:\n#         text (str): The input text containing the sections.\n#         section (str): The section to extract ('Helpful Answer' or 'Answer').\n        \n#     Returns:\n#         str: The content of the specified section or an empty string if the section is not found.\n#     \"\"\"\n#     # Define the pattern to match the section and capture the content after it\n#     pattern = rf\"{section}:\\s*(.+)\"\n    \n#     # Use regular expression to search for the section content\n#     match = re.search(pattern, text, re.IGNORECASE)\n    \n#     # Return the content if found, otherwise return an empty string\n#     return match.group(1).strip() if match else \"\"\n\n# # Example usage\n\n# from transformers import StoppingCriteria, StoppingCriteriaList\n\n# # define custom stopping criteria object\n# class StopOnTokens(StoppingCriteria):\n#     def _call_(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n#         for stop_ids in stop_token_ids:\n#             if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n#                 return True\n#         return False\n\n# def generate_prompt(user_question):\n#     query_template = \"\"\"\n#     Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n#     ### Instruction:\n#     For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n#     ### Input:\n#     {}\n#     \"\"\"\n    \n# def main():\n#     read_token='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'\n    \n \n#     user_secrets = UserSecretsClient()\n\n#     # hf_token = user_secrets.get_secret(\"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\")\n#     login(token = read_token)\n    \n  \n#     print(torch.cuda.is_available())\n#     print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")\n#     torch.cuda.empty_cache()\n    \n    \n\n#     # model_id = 'meta-llama/Llama-2-7b-chat-hf'\n#     # model_id = 'Undi95/Meta-Llama-3-8B-hf'\n#     # model_id = 'meta-llama/Meta-Llama-3-8B'\n#     model_id = 'OdiaGenAI/odiagenAI-bengali-base-model-v1'\n#     # model_id = 'describeai/gemini'\n\n#     device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n#     # set quantization configuration to load large model with less GPU memory\n#     # this requires the bitsandbytes library\n#     bnb_config = transformers.BitsAndBytesConfig(\n#         load_in_4bit=True,\n#         bnb_4bit_quant_type='nf4',\n#         bnb_4bit_use_double_quant=True,\n#         bnb_4bit_compute_dtype=bfloat16\n#     )\n\n#     # begin initializing HF items, you need an access token\n#     hf_auth = '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'\n#     model_config = transformers.AutoConfig.from_pretrained(\n#         model_id,\n#         use_auth_token=hf_auth\n#     )\n\n#     model = transformers.AutoModelForCausalLM.from_pretrained(\n#         model_id,\n#         trust_remote_code=True,\n#         config=model_config,\n#         quantization_config=bnb_config,\n#     #     device_map='auto',\n#         device_map={'': 0},\n#         use_auth_token=hf_auth\n#     )\n\n#     # enable evaluation mode to allow model inference\n#     model.eval()\n    \n    \n#     tokenizer = LlamaTokenizer.from_pretrained(model_id)\n\n#     stop_list = ['\\nHuman:', '\\n```\\n']\n\n#     stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n\n#     stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n    \n#     stop_token_ids = [x.cpu().tolist() for x in stop_token_ids]  # Convert to CPU and then to list\n\n    \n    \n\n#     stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n    \n#     generate_text = transformers.pipeline(\n#         model=model, \n#         tokenizer=tokenizer,\n#         return_full_text=True,  # langchain expects the full text\n#         task='text-generation',\n#         # we pass model parameters here too\n#         stopping_criteria=stopping_criteria,  # without this model rambles during chat\n#         temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n#         max_new_tokens=512,  # max number of tokens to generate in the output\n#         repetition_penalty=1.1  # without this output begins repeating\n#     )\n    \n    \n\n#     llm = HuggingFacePipeline(pipeline=generate_text)\n    \n    \n\n#     # Read the Excel file\n#     file_path = \"/kaggle/input/bangla-fin-gpt-data-updated/bangla-fin-qa-data.xlsx\"\n#     df = pd.read_excel(file_path)\n\n#     # Convert the DataFrame into a list of Documents with proper formatting\n#     documents = []\n\n#     for index, row in df.iterrows():\n#         # Extracting specific columns: Question, Answer, Context\n#         question = str(row['Question']) if 'Question' in row else ''\n#         answer = str(row['Answer']) if 'Answer' in row else ''\n#         context = str(row['Context']) if 'Context' in row else ''\n\n#         # Formatting the document content\n#         # content = f\"Question: {question}\\nAnswer: {answer}\\nContext: {context}\"\n#         content = f\"{context}\\n{question}\\n{answer}\\n\"\n\n\n#         # Create a Document object with formatted content\n#         document = Document(page_content=content)\n#         documents.append(document)\n\n#     from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n#     all_splits = text_splitter.split_documents(documents)\n    \n    \n#     from langchain.embeddings import HuggingFaceEmbeddings\n#     from langchain.vectorstores import FAISS\n\n#     model_name = \"sentence-transformers/all-mpnet-base-v2\"\n#     model_kwargs = {\"device\": \"cuda\"}\n\n#     embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n#     # storing embeddings in the vector store\n#     vectorstore = FAISS.from_documents(all_splits, embeddings)\n    \n#     from langchain.chains import ConversationalRetrievalChain\n\n#     chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n    \n#     st.title(\"Bangla Chatbot\")\n#     st.write(\"This chatbot generates meaningful responses in Bangla.\")\n\n#     # Initialize the chat history\n#     if \"messages\" not in st.session_state:\n#         st.session_state.messages = []\n\n#     # Display chat messgaes from history on app rerun\n#     for message in st.session_state.messages:\n#         with st.chat_message(message['role']):\n#             st.markdown(message['content'])\n\n   \n#     # Replace {} with the dynamic user question\n#     return query_template.format(user_question)\n\n#     # React to user input\n#     if prompt := st.chat_input(\"What is up?\"):\n#             # Display assistant response in chat message container\n#             with st.chat_message(\"user\"):\n#                 st.markdown(prompt)\n\n#             # Add user message to chat history\n#             st.session_state.messages.append({\"role\": \"question\", \"content\": prompt})\n\n#             result = chain({\"question\": generate_prompt(prompt), \"chat_history\": []})\n\n#             result_to_show = extract_content(result['answer'], 'Helpful Answer')\n#             # Display assistant response in chat message container\n#             with st.chat_message(\"assistant\"):\n#                 st.markdown(result_to_show);\n\n#             # Add assistant response to chat history\n#             st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\n        \n# if __name__ == '__main__':\n#     main()","metadata":{"_uuid":"ce855be5-7018-4356-acfd-d4c6761f7f1d","_cell_guid":"3d14e20d-88e4-4f47-ba28-94e62068f05e","trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:03.605022Z","iopub.execute_input":"2024-11-13T11:24:03.605386Z","iopub.status.idle":"2024-11-13T11:24:03.619491Z","shell.execute_reply.started":"2024-11-13T11:24:03.605353Z","shell.execute_reply":"2024-11-13T11:24:03.618447Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport re\nfrom torch import cuda, bfloat16\nimport transformers\nfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig\nfrom langchain_community.llms import HuggingFacePipeline\nimport pandas as pd\nfrom langchain.schema import Document\n\nimport torch\nimport torchvision\nimport transformers\nfrom transformers import AutoTokenizer\n\n\n\n# Function to extract content\ndef extract_content(text, section):\n    pattern = rf\"{section}:\\s*(.+)\"\n    match = re.search(pattern, text, re.IGNORECASE)\n    return match.group(1).strip() if match else \"\"\n\n# Stopping criteria\nfrom transformers import StoppingCriteria, StoppingCriteriaList\n\n\ndef generate_prompt(user_question):\n    query_template = \"\"\"\n    Below is an instruction that describes what you need to do for response, paired with an input that actually the question. Write a response that appropriately completes the request.\n\n    ### Instruction:\n    For generating meaningful responses in Bangla, maintain a detailed and precise approach. Avoid repeating the same words or phrases unnecessarily. Ensure that each response is unique and valuable, offering new insights or information. The content provided under \"Input\" serves as the core question, and it is crucial not to repeat or reiterate anything from the prompt or instructions given. Adhere strictly to the guidance provided, but do not replicate any wording directly. The answers should always be delivered in Bangla, without incorporating any English language elements. Avoid mirroring the given question in the response. The instructions should guide the chatbot to craft rich, informative, and contextually appropriate replies, enhancing the user experience by providing substantial and clear answers without redundancy.\n\n    ### Input:\n    {}\n    \"\"\"\n    return query_template.format(user_question)\n\n# Initialization of models, vectorstore, and chain (runs only once)\n@st.cache_resource(show_spinner=False)\ndef initialize_model_and_chain():\n    read_token = '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'\n    user_secrets = UserSecretsClient()\n    login(token=read_token)\n    torch.cuda.empty_cache()\n\n    # Model settings\n    model_id = 'Zafor158/BanglaLLama-3.2-3b-banglafingpt_finetuned-instruct-v0.0.1'\n    device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n    # Load the model with quantization\n    bnb_config = transformers.BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=bfloat16\n    )\n    \n    hf_auth = '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'\n    model_config = transformers.AutoConfig.from_pretrained(\n        model_id,\n        use_auth_token=hf_auth\n    )\n    \n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_id,\n        trust_remote_code=True,\n        config=model_config,\n        quantization_config=bnb_config,\n        device_map={'': 0},\n        use_auth_token=hf_auth\n    )\n\n    # tokenizer = LlamaTokenizer.from_pretrained(model_id)\n    tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=hf_auth)\n\n\n    # Stopping tokens\n    stop_list = ['\\nHuman:', '\\n```\\n']\n    stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n    stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n    stop_token_ids = [x.cpu().tolist() for x in stop_token_ids]\n\n#     class StopOnTokens(StoppingCriteria):\n#         def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n#             for stop_ids in stop_token_ids:\n#                 if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n#                     return True\n#             return False\n    class StopOnTokens(StoppingCriteria):\n        def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n            # Loop through the stop token IDs and check if they appear in the current input_ids\n            for stop_ids in stop_token_ids:\n                stop_ids_tensor = torch.tensor(stop_ids, device=input_ids.device)  # Convert list to tensor\n                if torch.eq(input_ids[0][-len(stop_ids):], stop_ids_tensor).all():\n                    return True\n            return False\n\n    \n    stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n\n    # Text generation pipeline\n    generate_text = transformers.pipeline(\n        model=model, \n        tokenizer=tokenizer,\n        return_full_text=True,\n        task='text-generation',\n        stopping_criteria=stopping_criteria,\n        temperature=0.1,\n        max_new_tokens=512,\n        repetition_penalty=1.1\n    )\n\n    llm = HuggingFacePipeline(pipeline=generate_text)\n\n    # Loading and preparing the data\n    file_path = \"/kaggle/input/bangla-fin-gpt-data-updated/bangla-fin-qa-data.xlsx\"\n    df = pd.read_excel(file_path)\n\n    # Convert the DataFrame into a list of Documents\n    documents = []\n    for index, row in df.iterrows():\n        question = str(row['Question']) if 'Question' in row else ''\n        answer = str(row['Answer']) if 'Answer' in row else ''\n        context = str(row['Context']) if 'Context' in row else ''\n        content = f\"{context}\\n{question}\\n{answer}\\n\"\n        document = Document(page_content=content)\n        documents.append(document)\n\n    # Splitting documents\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n    all_splits = text_splitter.split_documents(documents)\n\n    # Embeddings and FAISS\n    from langchain.embeddings import HuggingFaceEmbeddings\n    from langchain.vectorstores import FAISS\n    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n    model_kwargs = {\"device\": \"cuda\"}\n    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n\n    # Store embeddings in the vectorstore\n    vectorstore = FAISS.from_documents(all_splits, embeddings)\n\n    # Create conversational retrieval chain (only once)\n    from langchain.chains import ConversationalRetrievalChain\n    chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n\n    return chain\n\n# Main function\ndef main():\n    st.title(\"Bangla Chatbot\")\n    st.write(\"This chatbot generates meaningful responses in Bangla.\")\n\n    # Initialize chain (only runs once)\n    chain = initialize_model_and_chain()\n\n    # Initialize the chat history\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    # Display chat messages from history on app rerun\n    for message in st.session_state.messages:\n        with st.chat_message(message['role']):\n            st.markdown(message['content'])\n\n    # React to user input\n    if prompt := st.chat_input(\"What is up?\"):\n        # Display user message in chat message container\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n\n        # Add user message to chat history\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n        # Conversational chain logic (no need to create chain every time)\n        result = chain({\"question\": generate_prompt(prompt), \"chat_history\": []})\n\n        # Extract response\n        result_to_show = extract_content(result['answer'], 'Helpful Answer')\n\n        # Display assistant response in chat message container\n        with st.chat_message(\"assistant\"):\n            st.markdown(result_to_show)\n\n        # Add assistant response to chat history\n        st.session_state.messages.append({\"role\": \"assistant\", \"content\": result_to_show})\n\nif __name__ == '__main__':\n    main()","metadata":{"_uuid":"36f622ac-b111-41fd-93ca-5452750f857f","_cell_guid":"af1b9e19-fcf4-4b8c-af6e-ef0c6605d10a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:03.621305Z","iopub.execute_input":"2024-11-13T11:24:03.621636Z","iopub.status.idle":"2024-11-13T11:24:03.639037Z","shell.execute_reply.started":"2024-11-13T11:24:03.621586Z","shell.execute_reply":"2024-11-13T11:24:03.638079Z"}},"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\\n  | tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" \\\n  | tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && apt install ngrok","metadata":{"_uuid":"770ce467-d8b0-444e-8df5-3a140ef14446","_cell_guid":"628d3808-2845-4043-951c-f77c0f98c370","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:03.640473Z","iopub.execute_input":"2024-11-13T11:24:03.640864Z","iopub.status.idle":"2024-11-13T11:24:14.541805Z","shell.execute_reply.started":"2024-11-13T11:24:03.640822Z","shell.execute_reply":"2024-11-13T11:24:14.540626Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"deb https://ngrok-agent.s3.amazonaws.com buster main\nGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\nGet:2 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]          \u001b[0m\u001b[33m\nHit:3 http://archive.ubuntu.com/ubuntu jammy InRelease                         \u001b[0m\u001b[33m\nGet:4 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]   \nGet:5 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]       \u001b[0m\u001b[33m\u001b[33m\nGet:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]m       \u001b[0m\u001b[33m\nGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1107 kB]\nHit:8 https://packages.cloud.google.com/apt google-fast-socket InRelease       \u001b[0m\u001b[33m\nGet:9 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \u001b[0m\nGet:10 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [7047 B]\u001b[0m\u001b[33m\nGet:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]33m  \u001b[0m\u001b[33m\u001b[33m\nGet:12 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [28.6 kB]\nGet:13 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\nGet:14 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3410 kB]\nGet:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2700 kB]\u001b[33m\u001b[33m\nGet:16 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1576 kB]\nGet:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1164 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3353 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1452 kB]m\nGet:20 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.7 kB]\nGet:21 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]3m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\nGet:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2424 kB]\nGet:23 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3275 kB]\nFetched 21.0 MB in 2s (8705 kB/s)    \u001b[0m                   \u001b[0m\u001b[33m\u001b[33m\u001b[33m\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\n77 packages can be upgraded. Run 'apt list --upgradable' to see them.\n\u001b[1;33mW: \u001b[0mhttps://packages.cloud.google.com/apt/dists/gcsfuse-focal/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n\u001b[1;33mW: \u001b[0mhttps://packages.cloud.google.com/apt/dists/google-fast-socket/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  ngrok\n0 upgraded, 1 newly installed, 0 to remove and 77 not upgraded.\nNeed to get 9885 kB of archives.\nAfter this operation, 0 B of additional disk space will be used.\nGet:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.18.4 [9885 kB]\nFetched 9885 kB in 0s (36.2 MB/s)0m\u001b[33m\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package ngrok.\n(Reading database ... 122996 files and directories currently installed.)\nPreparing to unpack .../ngrok_3.18.4_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking ngrok (3.18.4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up ngrok (3.18.4) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"!pip install --quiet pyngrok\n!pip install --no-dependencies --quiet protobuf==3.20.*   #==4.21.12!pip install --no-dependencies --quiet validators\n!pip install --no-dependencies --quiet validators","metadata":{"_uuid":"104898ae-5bee-4126-83a9-0ea46f1f162f","_cell_guid":"6589cb94-4ec5-44e8-8500-97c75d7c8273","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:14.543379Z","iopub.execute_input":"2024-11-13T11:24:14.543723Z","iopub.status.idle":"2024-11-13T11:24:30.812720Z","shell.execute_reply.started":"2024-11-13T11:24:14.543689Z","shell.execute_reply":"2024-11-13T11:24:30.811448Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"!ngrok config add-authtoken 2m3kux5R5Tqcfm2NgpjeWiADTKC_6JyUqUUyA1B6F1nUE5MQg","metadata":{"_uuid":"a9ac6898-83c1-4c81-8464-1593efad9a23","_cell_guid":"937c2090-8ecd-48b1-b3ff-f6d2bc686f01","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:30.814848Z","iopub.execute_input":"2024-11-13T11:24:30.815826Z","iopub.status.idle":"2024-11-13T11:24:33.081881Z","shell.execute_reply.started":"2024-11-13T11:24:30.815776Z","shell.execute_reply":"2024-11-13T11:24:33.080596Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# !streamlit run --server.port 80 my_app.py > /dev/null\n\nimport subprocess\nsubprocess.Popen([\"streamlit\", \"run\", \"--server.port\", \"80\", \"app.py\"])","metadata":{"_uuid":"5c545bb5-2132-489b-9adc-2aa62d80844c","_cell_guid":"159b1d89-deea-4c13-9e1e-3ec9c609a57f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:33.083471Z","iopub.execute_input":"2024-11-13T11:24:33.083819Z","iopub.status.idle":"2024-11-13T11:24:33.092922Z","shell.execute_reply.started":"2024-11-13T11:24:33.083784Z","shell.execute_reply":"2024-11-13T11:24:33.091638Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"<Popen: returncode: None args: ['streamlit', 'run', '--server.port', '80', '...>"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"# !pip install watchdog","metadata":{"_uuid":"ca73ee4f-d583-4e7a-99e4-d768f99e876b","_cell_guid":"264fbca0-7224-4ce6-9aab-5f1729570b8a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:33.094147Z","iopub.execute_input":"2024-11-13T11:24:33.094509Z","iopub.status.idle":"2024-11-13T11:24:33.102587Z","shell.execute_reply.started":"2024-11-13T11:24:33.094467Z","shell.execute_reply":"2024-11-13T11:24:33.101666Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"!ngrok http 80","metadata":{"_uuid":"999bcc76-704b-4226-92a3-18d02e09180c","_cell_guid":"18b4b699-2584-4f4c-b51a-497eeafd600c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T11:24:33.103928Z","iopub.execute_input":"2024-11-13T11:24:33.104388Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\n\n  You can now view your Streamlit app in your browser.\n\n  Local URL: http://localhost:80\n  Network URL: http://172.19.2.2:80\n  External URL: http://34.82.103.168:80\n\n\u001b7\u001b[?47h\u001b[?1h\u001b=\u0002\u0007\u001b[H\u001b[2J\u001b[m\u001b[38;5;6m\u001b[48;5;16m\u001b[1m\u001b[1;1Hngrok\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                                           \u001b[m\u001b[38;5;7m\u001b[48;5;16m(Ctrl+C to quit)\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[2;1H                                                                                \u001b[m\u001b[38;5;6m\u001b[48;5;16m\u001b[3;1HSession Status                connecting\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                        \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[4;1HVersion                       3.18.4\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                            \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[5;1HWeb Interface                 http://127.0.0.1:4040\u001b[m\u001b[38;5;16m\u001b[48;5;16m                             \u001b[6;1H                                                                                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[7;1HConnections                   ttl     opn     rt1     rt5     p50     p90     \u001b[m\u001b[38;5;16m\u001b[48;5;16m  \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[8;1H                              0       0       0.00    0.00    0.00    0.00    \u001b[m\u001b[38;5;16m\u001b[48;5;16m  \u001b[9;1H                                                                                \u001b[10;1H                                                                                \u001b[11;1H                                                                                \u001b[12;1H                                                                                \u001b[13;1H                                                                                \u001b[14;1H                                                                                \u001b[15;1H                                                                                \u001b[16;1H                                                                                \u001b[17;1H                                                                                \u001b[18;1H                                                                                \u001b[19;1H                                                                                \u001b[20;1H                                                                                \u001b[21;1H                                                                                \u001b[22;1H                                                                                \u001b[23;1H                                                                                \u001b[24;1H                                                                                \u001b[m\u001b[38;5;5m\u001b[48;5;16m\u001b[3;1HTake our ngrok in production survey! https://forms.gle/aXiBFWzEA36DudFn6\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[4;1H                                    \u001b[m\u001b[38;5;2m\u001b[48;5;16m\u001b[5;1HSession Status                online\u001b[m\u001b[38;5;16m\u001b[48;5;16m               \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[6;1HAccount                       ornonornob@gmail.com (Plan: Free)\u001b[7;1HVersion    \u001b[7;31H3.18.4\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                          \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[8;1HRegion\u001b[8;31HUnited\u001b[8;38HStates\u001b[8;45H(us)\u001b[m\u001b[38;5;16m\u001b[48;5;16m                              \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[9;1HWeb Interface                 http://127.0.0.1:4040\u001b[10;1HForwarding                    https://1656-34-82-103-168.ngrok-free.app -> http:\u001b[12;1HConnections                   ttl     opn     rt1     rt5     p50     p90     \u001b[13;1H                              0       0       0.00    0.00    0.00    0.00    \u001b[9;1HLatency      \u001b[9;31H67ms\u001b[m\u001b[38;5;16m\u001b[48;5;16m                 \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[10;1HWeb Interface\u001b[10;35H:/\u001b[10;38H127.0.0.1:404\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[10;52H                             \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[11;1HForwarding                    https://1656-34-82-103-168.ngrok-free.app -> http:\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[12;1H                                                                              \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[13;1HConnections\u001b[13;31Httl\u001b[13;39Hopn\u001b[13;47Hrt1 \u001b[13;55Hrt5 \u001b[13;63Hp5\u001b[13;66H \u001b[13;71Hp9\u001b[13;74H \u001b[14;1H                              0       0       0.00    0.00    0.00    0.00    \u001b[9;31H71\u001b[9;31H67\u001b[9;31H74\u001b[9;31H69\u001b[9;32H7\u001b[14;39H1\u001b[16;1HHTTP Requests\u001b[17;1H-------------\u001b[19;1H11:30:01.862 UTC\u001b[19;18HGET\u001b[19;22H/\u001b[19;49H200 OK\u001b[19;8H2\u001b[19;10H257\u001b[19;23Hstatic/media/SourceSansPro-Regular.0d69e5ff5e92ac64a0c9.wo\u001b[20;1H11:30:01.862 UTC\u001b[20;18HGET\u001b[20;22H/\u001b[14;39H2\u001b[20;8H2\u001b[20;10H4\u001b[20;12H9\u001b[20;23Hstatic/css/main.23bdda6f.css\u001b[21;1H11:30:01.862 UTC\u001b[21;18HGET\u001b[21;22H/\u001b[14;39H5\u001b[20;10H526\u001b[20;30Hmedia/SourceSansPro-Bold.118dea98980e20a81ced.woff2\u001b[21;8H2\u001b[21;10H4\u001b[21;12H9\u001b[21;23Hstatic/css/main.23bdda6f.css\u001b[22;1H11:30:02.526 UTC\u001b[22;18HGET\u001b[22;22H/static/media/SourceSansPro-SemiBold.abed79cd0df1827e18cf.w\u001b[23;1H11:30:01.862 UTC\u001b[23;18HGET\u001b[23;22H/\u001b[19;10H526\u001b[19;30Hjs/m\u001b[19;35Hin.75ac1cb6.js\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[20;10H257\u001b[20;50HRegular.0d6\u001b[20;62He5ff5e92ac64a0c9.wo\u001b[21;10H526\u001b[21;30Hmedia/SourceSansPro-Bold.118dea98980e20a81ced.woff2\u001b[22;10H469\u001b[22;30Hcss/main.23bdda6f.css\u001b[m\u001b[38;5;16m\u001b[48;5;16m                              \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[23;8H2\u001b[23;10H526\u001b[23;23Hstatic/media/SourceSansPro-SemiBold.abed79cd0df1827e18cf.w\u001b[24;1H11:30:01.862 UTC\u001b[24;18HGET\u001b[24;22H/\u001b[14;31H1\u001b[14;39H4\u001b[14;65H67\u001b[14;73H67\u001b[14;31H2\u001b[14;39H3\u001b[14;65H74\u001b[14;73H80\u001b[14;50H3\u001b[14;58H1\u001b[14;31H3\u001b[14;39H2\u001b[14;65H80\u001b[14;71H2\u001b[14;73H33\u001b[19;8H6\u001b[19;10H064\u001b[19;23H_stcore/health\u001b[m\u001b[38;5;16m\u001b[48;5;16m            \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[20;10H526\u001b[20;30Hjs/m\u001b[20;35Hin.75ac1cb6.js\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[21;10H257\u001b[21;50HRegular.0d6\u001b[21;62He5ff5e92ac64a0c9.wo\u001b[22;10H526\u001b[22;30Hmedia/SourceSansPro-Bold.118dea98980e20a81ced.woff2\u001b[23;10H469\u001b[23;30Hcss/main.23bdda6f.css\u001b[m\u001b[38;5;16m\u001b[48;5;16m                              \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[24;8H2\u001b[24;10H526\u001b[24;23Hstatic/media/SourceSansPro-SemiBold.abed79cd0df1827e18cf.w\u001b[20;8H6\u001b[20;10H064\u001b[20;23H_stcore/host-config\u001b[m\u001b[38;5;16m\u001b[48;5;16m       \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[21;10H526\u001b[21;30Hjs/m\u001b[21;35Hin.75ac1cb6.js\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[22;10H257\u001b[22;50HRegular.0d6\u001b[22;62He5ff5e92ac64a0c9.wo\u001b[23;10H526\u001b[23;30Hmedia/SourceSansPro-Bold.118dea98980e20a81ced.woff2\u001b[24;10H469\u001b[24;30Hcss/main.23bdda6f.css\u001b[m\u001b[38;5;16m\u001b[48;5;16m                              \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[14;39H3\u001b[19;10H306\u001b[19;23Hfavicon.png\u001b[m\u001b[38;5;16m\u001b[48;5;16m   \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[20;32Health\u001b[m\u001b[38;5;16m\u001b[48;5;16m     \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[21;8H6\u001b[21;10H064\u001b[21;23H_stcore/host-config\u001b[m\u001b[38;5;16m\u001b[48;5;16m       \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[22;10H526\u001b[22;30Hjs/m\u001b[22;35Hin.75ac1cb6.js\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[23;10H257\u001b[23;50HRegular.0d6\u001b[23;62He5ff5e92ac64a0c9.wo\u001b[24;10H526\u001b[24;30Hmedia/SourceSansPro-Bold.118dea98980e20a81ced.woff2\u001b[14;31H4\u001b[14;39H2\u001b[14;65H74\u001b[14;39H3\u001b[20;10H825\u001b[20;31Hstream\u001b[21;32Health\u001b[m\u001b[38;5;16m\u001b[48;5;16m     \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[22;8H6\u001b[22;10H064\u001b[22;23H_stcore/host-config\u001b[m\u001b[38;5;16m\u001b[48;5;16m       \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[23;10H526\u001b[23;30Hjs/m\u001b[23;35Hin.75ac1cb6.js\u001b[m\u001b[38;5;16m\u001b[48;5;16m                                \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[24;10H257\u001b[24;50HRegular.0d6\u001b[24;62He5ff5e92ac64a0c9.wo\u001b[19;75H200 OK\u001b[20;75H101 Sw\u001b[21;75H200 OK\u001b[22;75H200 OK\u001b[23;75H200 OK\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[24;74H \u001b[m\u001b[38;5;7m\u001b[48;5;16m200 OK\u001b[14;50H6The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:991: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:182: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[9;32H6","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[14;50H5","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/app.py:116: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline=generate_text)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[14;50H4","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/app.py:138: LangChainDeprecationWarning: Importing HuggingFaceEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n\n>> from langchain.embeddings import HuggingFaceEmbeddings\n\nwith new imports of:\n\n>> from langchain_community.embeddings import HuggingFaceEmbeddings\nYou can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n  from langchain.embeddings import HuggingFaceEmbeddings\n/kaggle/working/app.py:139: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n\n>> from langchain.vectorstores import FAISS\n\nwith new imports of:\n\n>> from langchain_community.vectorstores import FAISS\nYou can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n  from langchain.vectorstores import FAISS\n/kaggle/working/app.py:142: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[14;50H3\u001b[14;50H2\u001b[14;50H1\u001b[14;31H6\u001b[14;39H1\u001b[14;63H1\u001b[14;65H56\u001b[14;71H94.47\u001b[14;50H5\u001b[14;58H2\u001b[14;50H4\u001b[14;50H3\u001b[9;31H72\u001b[14;58H1\u001b[9;31H66\u001b[14;50H2\u001b[14;50H1\u001b[9;31H70\u001b[9;31H66\u001b[14;50H0\u001b[9;32H7\u001b[9;32H6\u001b[9;31H78\u001b[9;31H66\u001b[9;31H70\u001b[9;31H66\u001b[9;31H74\u001b[9;31H66\u001b[14;58H0\u001b[9;31H82\u001b[9;31H66\u001b[9;31H77\u001b[9;31H66\u001b[9;31H81\u001b[9;31H67\u001b[9;32H6\u001b[9;32H7\u001b[9;32H6\u001b[9;32H7\u001b[9;32H6\u001b[9;32H8\u001b[9;32H6\u001b[9;31H84\u001b[9;31H66\u001b[9;31H83\u001b[9;31H66\u001b[9;31H88\u001b[9;31H66\u001b[9;31H107ms\u001b[9;31H66ms\u001b[m\u001b[38;5;16m\u001b[48;5;16m \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[9;32H7\u001b[9;32H6\u001b[9;32H7\u001b[9;32H6\u001b[9;32H8\u001b[9;32H6\u001b[9;31H70\u001b[9;31H66\u001b[9;31H77\u001b[9;31H68\u001b[9;31H70\u001b[9;31H66\u001b[9;32H8\u001b[9;32H6","output_type":"stream"},{"name":"stderr","text":"2024-11-13 12:06:08.265 Examining the path of torch.classes raised: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n","output_type":"stream"},{"name":"stdout","text":"\u001b[14;39H2\u001b[19;2H2\u001b[19;4H06\u001b[19;8H9\u001b[19;10H718\u001b[19;23Hstatic/js/1086.93ecee4c.chunk.js\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[19;75H      \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[20;10H306\u001b[20;23Hfavicon.png\u001b[m\u001b[38;5;16m\u001b[48;5;16m   \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[20;75H2\u001b[20;77H0\u001b[20;79HOK\u001b[21;10H825\u001b[21;31Hstream\u001b[21;75H1\u001b[21;77H1\u001b[21;79HSw\u001b[22;32Health\u001b[m\u001b[38;5;16m\u001b[48;5;16m     \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[23;8H6\u001b[23;10H064\u001b[23;23H_stcore/host-config\u001b[m\u001b[38;5;16m\u001b[48;5;16m       \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[24;10H526\u001b[24;30Hjs/m\u001b[24;35Hin.75ac1cb6.js\u001b[m\u001b[38;5;16m\u001b[48;5;16m                         \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[19;75H200 OK\u001b[14;39H3\u001b[19;11H84\u001b[19;33H8648\u001b[19;39H2079acf\u001b[m\u001b[38;5;16m\u001b[48;5;16m\u001b[19;75H      \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[20;2H2\u001b[20;4H06\u001b[20;8H9\u001b[20;10H718\u001b[20;23Hstatic/js/1086.93ecee4c.chunk.js\u001b[21;10H306\u001b[21;23Hfavicon.png\u001b[m\u001b[38;5;16m\u001b[48;5;16m   \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[21;75H2\u001b[21;77H0\u001b[21;79HOK\u001b[22;10H825\u001b[22;31Hstream\u001b[22;75H1\u001b[22;77H1\u001b[22;79HSw\u001b[23;32Health\u001b[m\u001b[38;5;16m\u001b[48;5;16m     \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[24;8H6\u001b[24;10H064\u001b[24;23H_stcore/host-config\u001b[m\u001b[38;5;16m\u001b[48;5;16m       \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[19;75H200 OK\u001b[9;31H74\u001b[9;31H66\u001b[14;31H8\u001b[14;39H1\u001b[14;63H46.30\u001b[14;50H3\u001b[14;58H1\u001b[9;31H82\u001b[14;50H2\u001b[9;31H66\u001b[14;50H1\u001b[14;58H0\u001b[9;31H72\u001b[9;31H66\u001b[14;50H0\u001b[9;31H77\u001b[9;31H66\u001b[9;32H3\u001b[9;32H5\u001b[9;32H3\u001b[9;31H75\u001b[9;31H63\u001b[9;31H74\u001b[9;31H63\u001b[9;31H8\u001b[9;31H6\u001b[9;31H85\u001b[9;31H63\u001b[9;31H77\u001b[9;31H66\u001b[9;31H78\u001b[9;31H66","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/app.py:180: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  result = chain({\"question\": generate_prompt(prompt), \"chat_history\": []})\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[9;31H71\u001b[9;31H80\u001b[9;31H68\u001b[9;32H6\u001b[9;31H79\u001b[9;31H66\u001b[9;31H88\u001b[9;31H66\u001b[9;31H7\u001b[9;31H6\u001b[9;31H73\u001b[9;31H66\u001b[9;31H92\u001b[9;31H66\u001b[9;32H7\u001b[9;32H6\u001b[9;31H80\u001b[9;31H66\u001b[14;39H2\u001b[19;4H34\u001b[19;7H25\u001b[19;10H617\u001b[19;30Hc\u001b[19;32Hs/main.\u001b[19;40H3bdda6f.css\u001b[m\u001b[38;5;16m\u001b[48;5;16m    \u001b[19;75H      \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[20;11H84\u001b[20;33H8648\u001b[20;39H2079acf\u001b[21;2H2\u001b[21;4H06\u001b[21;8H9\u001b[21;10H718\u001b[21;23Hstatic/js/1086.93ecee4c.chunk.js\u001b[22;10H306\u001b[22;23Hfavicon.png\u001b[m\u001b[38;5;16m\u001b[48;5;16m   \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[22;75H2\u001b[22;77H0\u001b[22;79HOK\u001b[23;10H825\u001b[23;31Hstream\u001b[23;75H1\u001b[23;77H1\u001b[23;79HSw\u001b[24;32Health\u001b[m\u001b[38;5;16m\u001b[48;5;16m     \u001b[m\u001b[38;5;7m\u001b[48;5;16m\u001b[19;75H200 OK\u001b[14;31H9\u001b[14;39H1\u001b[14;63H90\u001b[14;66H27\u001b[14;50H2\u001b[14;50H1\u001b[9;32H7\u001b[9;32H6\u001b[14;50H0\u001b[9;31H77\u001b[9;32H8\u001b[9;32H5\u001b[9;31H66\u001b[9;31H70\u001b[9;32H1\u001b[9;31H66\u001b[9;31H83\u001b[9;31H66\u001b[9;31H72\u001b[9;31H66\u001b[9;31H78\u001b[9;31H67\u001b[9;31H85\u001b[9;31H66\u001b[9;31H72\u001b[9;31H66\u001b[9;31H70\u001b[9;31H97\u001b[9;31H66\u001b[9;32H3\u001b[9;32H4\u001b[9;32H3\u001b[9;32H7\u001b[9;32H3\u001b[9;32H5\u001b[9;32H3\u001b[9;31H74\u001b[9;31H63\u001b[9;31H70\u001b[9;31H63\u001b[9;32H4\u001b[9;32H3\u001b[9;31H88\u001b[9;31H63\u001b[9;32H4\u001b[9;32H3\u001b[9;31H76\u001b[9;31H63\u001b[9;31H71\u001b[9;31H63\u001b[9;32H5\u001b[9;32H3\u001b[9;32H4\u001b[9;32H3\u001b[9;32H5\u001b[9;32H3\u001b[9;31H74\u001b[9;31H63\u001b[9;32H7\u001b[9;32H3\u001b[9;31H72\u001b[9;31H63\u001b[9;32H7\u001b[9;32H3\u001b[9;32H4\u001b[9;32H3\u001b[9;32H5\u001b[9;32H3\u001b[9;32H4\u001b[9;32H3\u001b[9;32H7\u001b[9;32H3\u001b[9;31H75\u001b[9;31H63\u001b[9;31H75\u001b[9;31H63\u001b[9;32H4\u001b[9;32H3\u001b[9;31H78\u001b[9;32H4\u001b[9;31H63\u001b[9;31H70\u001b[9;31H69\u001b[9;32H3\u001b[9;31H74\u001b[9;31H63\u001b[9;31H80\u001b[9;31H63\u001b[9;32H5\u001b[9;32H3\u001b[9;31H92\u001b[9;31H63\u001b[9;31H77\u001b[9;31H63\u001b[9;31H70\u001b[9;31H63  Stopping...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"641cb1f8-f4e8-41ea-a8ea-6e71575ff6d0","_cell_guid":"fa9f07c4-3cd9-43e7-aa29-239b85d47add","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"c6b49fb1-e96c-4f84-8189-418f99fea340","_cell_guid":"cad1bdaa-281a-4f0d-8313-c8d96ae29960","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!streamlit run app.py &>/content/logs.txt & curl ipv4.icanhazip.com","metadata":{"_uuid":"70730324-1781-4342-9b2d-0589b7a47635","_cell_guid":"9aafac5e-393e-4b42-86ea-0fe4f985e0ab","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    'import sys, json; print(\"Execute the next cell and the go to the following URL: \" +json.load(sys.stdin)[\"tunnels\"][0][\"public_url\"])'","metadata":{"_uuid":"184d2649-ef93-4dda-8290-066038434369","_cell_guid":"146afb03-a05e-441f-aa22-c008eb380375","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run your file containing the script \n\n# your uploaded file\n!streamlit run ./eda_basketball.py\n\n# or\n\n# newly created file\n# !streamlit run ./my_app.py","metadata":{"_uuid":"33d4428d-ba8c-41dd-9110-33663cf1de30","_cell_guid":"ad120a96-6d01-4df3-9dbe-1708151ee8e1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !npx localtunnel --port 8501\n# !npx localtunnel --port 8501 --subdomain mycustomsubdomain","metadata":{"_uuid":"6d701c17-77e9-47e8-9ac3-6ede03aa8935","_cell_guid":"1750195e-15e8-41a2-abb5-c8ae2ffa9e28","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"b4791b04-d3cd-46a5-a2fb-be3b8f126b08","_cell_guid":"b71c1252-e33e-4eed-b760-82d7baf0f5c1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" [code] {\"jupyter\":{\"outputs_hidden\":false}}","metadata":{"_uuid":"1903690e-cc48-4f32-a0d9-143b5dbfc979","_cell_guid":"df787bd0-9aac-44fa-afb9-0c06b13f6165","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}